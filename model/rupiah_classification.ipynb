{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d16875af",
   "metadata": {},
   "source": [
    "## 1. Import Library\n",
    "\n",
    "Import seluruh library yang diperlukan. Library-library ini mencakup framework tensorflow untuk deep learning, tools preprocessing data dan augmentasi gambar, serta berbagai library untuk visualisasi dan evaluasi performa model. \n",
    "\n",
    "Jika belum menginstall library yang diperlukan, run: pip install ipykernel -r lib_requirement.txt\n",
    "lalu install di venv dengan py -m ipykernel install --name=<nama_venv>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bcb7f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library untuk manipulasi data dan sistem operasi\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from collections import Counter\n",
    "\n",
    "# library untuk visualisasi\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# library untuk image processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# library tensorflow dan keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# library untuk evaluasi model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# set random seed untuk reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# konfigurasi visualisasi\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"tensorflow version: {tf.__version__}\")\n",
    "print(f\"gpu available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c1f3a3",
   "metadata": {},
   "source": [
    "## 2. Data Collecting\n",
    "\n",
    "Dataset diambil dari kumpulan dataset uang rupiah kertas dan koin dengan karakteristik yang mirip.\n",
    "\n",
    "Analisis struktur dataset untuk memahami karakteristik data yang tersedia. Analisis mencakup perhitungan jumlah gambar per kelas, identifikasi ketidakseimbangan distribusi data, dan pemeriksaan properti gambar seperti dimensi dan format file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923acfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definisi path dataset\n",
    "base_dir = Path(r'c:\\Users\\Farre\\projects\\rupiah-classification-model\\data')\n",
    "train_dir = base_dir / 'train'\n",
    "valid_dir = base_dir / 'valid'\n",
    "test_dir = base_dir / 'test'\n",
    "\n",
    "# nama kelas nominal rupiah\n",
    "class_names = sorted([\n",
    "    'koin_100', 'koin_200', 'koin_500', 'koin_1000',\n",
    "    'kertas_1000', 'kertas_2000', 'kertas_5000', 'kertas_10000',\n",
    "    'kertas_20000', 'kertas_50000', 'kertas_100000'\n",
    "])\n",
    "\n",
    "print(\"struktur dataset:\")\n",
    "print(f\"direktori base: {base_dir}\")\n",
    "print(f\"direktori training: {train_dir}\")\n",
    "print(f\"direktori validasi: {valid_dir}\")\n",
    "print(f\"direktori testing: {test_dir}\")\n",
    "print(f\"\\njumlah kelas: {len(class_names)}\")\n",
    "print(f\"daftar kelas: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf8e41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fungsi untuk menghitung jumlah gambar per kelas\n",
    "def count_images_per_class(directory):\n",
    "    \"\"\"\n",
    "    menghitung jumlah file gambar di setiap folder kelas\n",
    "    \n",
    "    args:\n",
    "        directory: path ke direktori yang berisi folder kelas\n",
    "    \n",
    "    returns:\n",
    "        dictionary dengan nama kelas sebagai key dan jumlah gambar sebagai value\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    for class_name in class_names:\n",
    "        class_path = directory / class_name\n",
    "        if class_path.exists():\n",
    "            counts[class_name] = len(list(class_path.glob('*')))\n",
    "        else:\n",
    "            counts[class_name] = 0\n",
    "    return counts\n",
    "\n",
    "# hitung jumlah gambar di setiap split\n",
    "train_counts = count_images_per_class(train_dir)\n",
    "valid_counts = count_images_per_class(valid_dir)\n",
    "test_counts = count_images_per_class(test_dir)\n",
    "\n",
    "# buat dataframe untuk analisis\n",
    "df_counts = pd.DataFrame({\n",
    "    'train': train_counts,\n",
    "    'valid': valid_counts,\n",
    "    'test': test_counts\n",
    "})\n",
    "\n",
    "df_counts['total'] = df_counts.sum(axis=1)\n",
    "df_counts['train_pct'] = (df_counts['train'] / df_counts['total'] * 100).round(2)\n",
    "\n",
    "print(\"\\njumlah gambar per kelas:\")\n",
    "print(df_counts)\n",
    "print(f\"\\ntotal gambar:\")\n",
    "print(f\"training: {df_counts['train'].sum()}\")\n",
    "print(f\"validasi: {df_counts['valid'].sum()}\")\n",
    "print(f\"testing: {df_counts['test'].sum()}\")\n",
    "print(f\"total keseluruhan: {df_counts['total'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analisis ketidakseimbangan dataset\n",
    "train_mean = df_counts['train'].mean()\n",
    "train_std = df_counts['train'].std()\n",
    "\n",
    "print(f\"\\nstatistik distribusi data training:\")\n",
    "print(f\"rata-rata gambar per kelas: {train_mean:.2f}\")\n",
    "print(f\"standar deviasi: {train_std:.2f}\")\n",
    "print(f\"minimum: {df_counts['train'].min()}\")\n",
    "print(f\"maksimum: {df_counts['train'].max()}\")\n",
    "\n",
    "# identifikasi kelas yang tidak seimbang (di bawah 70% rata-rata)\n",
    "threshold = train_mean * 0.7\n",
    "imbalanced_classes = df_counts[df_counts['train'] < threshold]\n",
    "\n",
    "print(f\"\\nkelas dengan data tidak seimbang (< {threshold:.0f} gambar):\")\n",
    "if len(imbalanced_classes) > 0:\n",
    "    for idx, row in imbalanced_classes.iterrows():\n",
    "        deficit = threshold - row['train']\n",
    "        print(f\"  {idx}: {row['train']} gambar (kurang {deficit:.0f} dari threshold)\")\n",
    "else:\n",
    "    print(\"  tidak ada kelas yang sangat tidak seimbang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde86026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analisis properti gambar (ukuran, format, dll)\n",
    "def analyze_image_properties(directory, sample_size=50):\n",
    "    \"\"\"\n",
    "    menganalisis properti gambar seperti dimensi, format, dan ukuran file\n",
    "    \n",
    "    args:\n",
    "        directory: path ke direktori dataset\n",
    "        sample_size: jumlah gambar yang akan dianalisis per kelas\n",
    "    \n",
    "    returns:\n",
    "        dataframe berisi statistik properti gambar\n",
    "    \"\"\"\n",
    "    properties = []\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        class_path = directory / class_name\n",
    "        if not class_path.exists():\n",
    "            continue\n",
    "            \n",
    "        images = list(class_path.glob('*'))[:sample_size]\n",
    "        \n",
    "        for img_path in images:\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    width, height = img.size\n",
    "                    properties.append({\n",
    "                        'class': class_name,\n",
    "                        'width': width,\n",
    "                        'height': height,\n",
    "                        'aspect_ratio': width / height,\n",
    "                        'format': img.format,\n",
    "                        'mode': img.mode,\n",
    "                        'size_kb': os.path.getsize(img_path) / 1024\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"error membaca {img_path}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(properties)\n",
    "\n",
    "# analisis properti gambar dari training set\n",
    "print(\"\\nmenganalisis properti gambar dari training set...\")\n",
    "df_properties = analyze_image_properties(train_dir, sample_size=50)\n",
    "\n",
    "print(f\"\\nstatistik dimensi gambar:\")\n",
    "print(df_properties[['width', 'height', 'aspect_ratio']].describe())\n",
    "\n",
    "print(f\"\\nformat gambar:\")\n",
    "print(df_properties['format'].value_counts())\n",
    "\n",
    "print(f\"\\nmode warna:\")\n",
    "print(df_properties['mode'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff022b",
   "metadata": {},
   "source": [
    "## 3. Data Analisis & Visualisasi\n",
    "\n",
    "Visualisasi untuk memahami distribusi dan karakteristik dataset secara mendalam. Pada bagian ini akan dibuat berbagai plot untuk menganalisis keseimbangan data, melihat sampel gambar dari setiap kelas, dan mengidentifikasi potensi masalah dalam dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825cc301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisasi distribusi dataset\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# plot 1: distribusi data training per kelas\n",
    "ax1 = axes[0, 0]\n",
    "df_counts['train'].plot(kind='bar', ax=ax1, color='steelblue')\n",
    "ax1.axhline(y=train_mean, color='r', linestyle='--', label=f'rata-rata: {train_mean:.0f}')\n",
    "ax1.axhline(y=threshold, color='orange', linestyle='--', label=f'threshold 70%: {threshold:.0f}')\n",
    "ax1.set_title('distribusi data training per kelas', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('kelas nominal rupiah')\n",
    "ax1.set_ylabel('jumlah gambar')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# plot 2: perbandingan train, valid, test\n",
    "ax2 = axes[0, 1]\n",
    "df_plot = df_counts[['train', 'valid', 'test']].T\n",
    "df_plot.plot(kind='bar', ax=ax2, width=0.8)\n",
    "ax2.set_title('perbandingan distribusi data train, valid, test', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('split dataset')\n",
    "ax2.set_ylabel('jumlah gambar')\n",
    "ax2.legend(title='kelas', bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2, fontsize=8)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "plt.setp(ax2.xaxis.get_majorticklabels(), rotation=0)\n",
    "\n",
    "# plot 3: proporsi tiap split\n",
    "ax3 = axes[1, 0]\n",
    "total_per_split = [df_counts['train'].sum(), df_counts['valid'].sum(), df_counts['test'].sum()]\n",
    "colors = ['steelblue', 'lightcoral', 'lightgreen']\n",
    "ax3.pie(total_per_split, labels=['train', 'valid', 'test'], autopct='%1.1f%%', \n",
    "        colors=colors, startangle=90)\n",
    "ax3.set_title('proporsi data train, valid, test', fontsize=14, fontweight='bold')\n",
    "\n",
    "# plot 4: heatmap distribusi\n",
    "ax4 = axes[1, 1]\n",
    "heatmap_data = df_counts[['train', 'valid', 'test']].T\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='d', cmap='YlOrRd', ax=ax4, cbar_kws={'label': 'jumlah gambar'})\n",
    "ax4.set_title('heatmap distribusi dataset', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('kelas nominal rupiah')\n",
    "ax4.set_ylabel('split dataset')\n",
    "plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3941b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisasi sampel gambar dari setiap kelas\n",
    "fig, axes = plt.subplots(11, 5, figsize=(20, 44))\n",
    "\n",
    "for idx, class_name in enumerate(class_names):\n",
    "    class_path = train_dir / class_name\n",
    "    images = list(class_path.glob('*'))[:5]\n",
    "    \n",
    "    for col, img_path in enumerate(images):\n",
    "        ax = axes[idx, col]\n",
    "        img = Image.open(img_path)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        if col == 0:\n",
    "            ax.set_title(f'{class_name}\\n{img.size[0]}x{img.size[1]}', \n",
    "                        fontsize=10, fontweight='bold', loc='left')\n",
    "        else:\n",
    "            ax.set_title(f'{img.size[0]}x{img.size[1]}', fontsize=8)\n",
    "\n",
    "plt.suptitle('sampel gambar dari setiap kelas nominal rupiah', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac821c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata lengkap dataset\n",
    "# pisahkan kelas berdasarkan jenis uang\n",
    "koin_classes = [class_name for class_name in class_names if 'koin' in class_name]\n",
    "kertas_classes = [class_name for class_name in class_names if 'kertas' in class_name]\n",
    "\n",
    "print(\"\\nringkasan metadata dataset:\")\n",
    "print(f\"\\ntipe jenis uang:\")\n",
    "print(f\"  koin: {len(koin_classes)} kelas\")\n",
    "print(f\"    {koin_classes}\")\n",
    "print(f\"  kertas: {len(kertas_classes)} kelas\")\n",
    "print(f\"    {kertas_classes}\")\n",
    "\n",
    "overall_stats = pd.DataFrame({\n",
    "    'metrik': ['total gambar', 'jumlah kelas', 'rata-rata per kelas', 'std deviasi', \n",
    "               'min per kelas', 'max per kelas', 'median per kelas'],\n",
    "    'train': [df_counts['train'].sum(), len(class_names), df_counts['train'].mean(),\n",
    "              df_counts['train'].std(), df_counts['train'].min(), df_counts['train'].max(),\n",
    "              df_counts['train'].median()],\n",
    "    'valid': [df_counts['valid'].sum(), len(class_names), df_counts['valid'].mean(),\n",
    "              df_counts['valid'].std(), df_counts['valid'].min(), df_counts['valid'].max(),\n",
    "              df_counts['valid'].median()],\n",
    "    'test': [df_counts['test'].sum(), len(class_names), df_counts['test'].mean(),\n",
    "             df_counts['test'].std(), df_counts['test'].min(), df_counts['test'].max(),\n",
    "             df_counts['test'].median()]\n",
    "})\n",
    "\n",
    "print(\"\\nstatistik keseluruhan:\")\n",
    "print(overall_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bf6079",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing & Augmentation\n",
    "\n",
    "Tahap Data Preprocessing dan Data Augmentation akan membantu membuat dataset lebih siap digunakan dan robust dengan mengatasi ketidakseimbangan dataset sehingga meningkatkan kemampuan generalisasi model. Kelas dengan sampel terbatas akan mendapatkan augmentasi yang lebih intensif untuk menyeimbangkan distribusi data training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebdc09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# konfigurasi preprocessing\n",
    "IMG_SIZE = (224, 224)  # ukuran input standar untuk cnn\n",
    "BATCH_SIZE = 32\n",
    "TARGET_SAMPLES = 500  # target jumlah gambar per kelas setelah augmentasi\n",
    "\n",
    "print(\"konfigurasi preprocessing:\")\n",
    "print(f\"ukuran gambar input: {IMG_SIZE}\")\n",
    "print(f\"batch size: {BATCH_SIZE}\")\n",
    "print(f\"target sampel per kelas: {TARGET_SAMPLES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aa76ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation untuk training dengan teknik yang lebih agresif\n",
    "# augmentasi ini membantu model untuk lebih robust terhadap variasi gambar\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # normalisasi pixel value ke range 0-1\n",
    "    rotation_range=20,  # rotasi random hingga 20 derajat\n",
    "    width_shift_range=0.2,  # shift horizontal hingga 20%\n",
    "    height_shift_range=0.2,  # shift vertikal hingga 20%\n",
    "    shear_range=0.2,  # shear transformation\n",
    "    zoom_range=0.2,  # zoom in/out random\n",
    "    horizontal_flip=True,  # flip horizontal random\n",
    "    brightness_range=[0.8, 1.2],  # variasi brightness\n",
    "    fill_mode='nearest'  # metode untuk mengisi pixel yang hilang\n",
    ")\n",
    "\n",
    "# untuk validasi dan testing, hanya normalisasi tanpa augmentasi\n",
    "# ini penting untuk evaluasi yang konsisten dan fair\n",
    "valid_test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "print(\"data augmentation configuration:\")\n",
    "print(\"training: rotasi, shift, shear, zoom, flip, brightness adjustment\")\n",
    "print(\"validation & test: hanya normalisasi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d0c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# buat data generator untuk training, validation, dan testing\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',  # multi-class classification\n",
    "    shuffle=True,  # shuffle data setiap epoch\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "valid_generator = valid_test_datagen.flow_from_directory(\n",
    "    valid_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,  # tidak shuffle untuk validasi\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test_generator = valid_test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,  # tidak shuffle untuk testing\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\ndata generator berhasil dibuat:\")\n",
    "print(f\"training samples: {train_generator.samples}\")\n",
    "print(f\"validation samples: {valid_generator.samples}\")\n",
    "print(f\"testing samples: {test_generator.samples}\")\n",
    "print(f\"\\nclass indices: {train_generator.class_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452e1852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hitung class weights untuk menangani ketidakseimbangan dataset\n",
    "# class weights memberikan bobot lebih tinggi pada kelas minority\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.arange(len(class_names)),\n",
    "    y=train_generator.classes\n",
    ")\n",
    "\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print(\"\\nclass weights untuk menangani imbalanced dataset:\")\n",
    "for idx, class_name in enumerate(class_names):\n",
    "    weight = class_weights_dict[idx]\n",
    "    count = train_counts[class_name]\n",
    "    print(f\"{class_name:15} | count: {count:3} | weight: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ee288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisasi sample hasil augmentasi\n",
    "print(\"\\nvisualisasi hasil data augmentation:\")\n",
    "\n",
    "# ambil satu batch dari generator\n",
    "sample_batch = next(train_generator)\n",
    "sample_images = sample_batch[0][:9]  # ambil 9 gambar pertama\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, img in enumerate(sample_images):\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].axis('off')\n",
    "    axes[idx].set_title(f'augmented sample {idx+1}', fontsize=10)\n",
    "\n",
    "plt.suptitle('contoh hasil data augmentation pada training set', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# reset generator setelah visualisasi\n",
    "train_generator.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f88861",
   "metadata": {},
   "source": [
    "## 5. Data Modeling\n",
    "\n",
    "Arsitektur model dirancang khusus dengan CNN untuk klasifikasi gambar uang rupiah. Model dibangun dari nol dengan lima blok konvolusi yang semakin dalam untuk ekstraksi fitur hierarkis, dilengkapi dengan batch normalization untuk stabilitas training dan dropout untuk mencegah overfitting/underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a401453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter untuk training\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 50  # akan dihentikan lebih awal jika tidak ada improvement\n",
    "\n",
    "print(\"hyperparameter training:\")\n",
    "print(f\"learning rate: {LEARNING_RATE}\")\n",
    "print(f\"max epochs: {EPOCHS}\")\n",
    "print(f\"optimizer: Adam\")\n",
    "print(f\"loss function: categorical crossentropy\")\n",
    "print(f\"metrics: accuracy, top-2 accuracy, precision, recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7187936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arsitektur model \n",
    "# 1. global average pooling untuk mengurangi parameter\n",
    "# 2. l2 regularization pada dense layers\n",
    "# 3. batch normalization setelah pooling\n",
    "# 4. dropout bertahap\n",
    "\n",
    "model = models.Sequential([\n",
    "    # input layer\n",
    "    layers.Input(shape=(224, 224, 3)),\n",
    "    \n",
    "    # convolutional block 1: fitur dasar (edge, texture)\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same', name='conv1_1'),\n",
    "    layers.BatchNormalization(name='bn1_1'),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same', name='conv1_2'),\n",
    "    layers.BatchNormalization(name='bn1_2'),\n",
    "    layers.MaxPooling2D((2, 2), name='pool1'),\n",
    "    layers.Dropout(0.2, name='dropout1'),  # reduced dari 0.25\n",
    "    \n",
    "    # convolutional block 2: fitur lebih kompleks\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2_1'),\n",
    "    layers.BatchNormalization(name='bn2_1'),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2_2'),\n",
    "    layers.BatchNormalization(name='bn2_2'),\n",
    "    layers.MaxPooling2D((2, 2), name='pool2'),\n",
    "    layers.Dropout(0.2, name='dropout2'),  # reduced dari 0.25\n",
    "    \n",
    "    # convolutional block 3: fitur abstrak tingkat menengah\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='conv3_1'),\n",
    "    layers.BatchNormalization(name='bn3_1'),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='conv3_2'),\n",
    "    layers.BatchNormalization(name='bn3_2'),\n",
    "    layers.MaxPooling2D((2, 2), name='pool3'),\n",
    "    layers.Dropout(0.3, name='dropout3'),\n",
    "    \n",
    "    # convolutional block 4: fitur abstrak tingkat tinggi\n",
    "    layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='conv4_1'),\n",
    "    layers.BatchNormalization(name='bn4_1'),\n",
    "    layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='conv4_2'),\n",
    "    layers.BatchNormalization(name='bn4_2'),\n",
    "    layers.MaxPooling2D((2, 2), name='pool4'),\n",
    "    layers.Dropout(0.3, name='dropout4'),\n",
    "    \n",
    "    # convolutional block 5: fitur paling abstrak dan spesifik\n",
    "    layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='conv5_1'),\n",
    "    layers.BatchNormalization(name='bn5_1'),\n",
    "    layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='conv5_2'),\n",
    "    layers.BatchNormalization(name='bn5_2'),\n",
    "    layers.MaxPooling2D((2, 2), name='pool5'),\n",
    "    layers.Dropout(0.4, name='dropout5'),\n",
    "    \n",
    "    # global average pooling\n",
    "    layers.GlobalAveragePooling2D(name='global_avg_pool'),\n",
    "    layers.BatchNormalization(name='bn_global'),\n",
    "    \n",
    "    # layering dengan l2 regularization\n",
    "    layers.Dense(512, activation='relu', \n",
    "                kernel_regularizer=keras.regularizers.l2(0.001),\n",
    "                name='fc1'),\n",
    "    layers.BatchNormalization(name='bn_fc1'),\n",
    "    layers.Dropout(0.5, name='dropout_fc1'),\n",
    "    \n",
    "    layers.Dense(256, activation='relu',\n",
    "                kernel_regularizer=keras.regularizers.l2(0.001),\n",
    "                name='fc2'),\n",
    "    layers.BatchNormalization(name='bn_fc2'),\n",
    "    layers.Dropout(0.3, name='dropout_fc2'),  # reduced dari 0.5\n",
    "    \n",
    "    # output layer: 11 kelas nominal rupiah\n",
    "    layers.Dense(len(class_names), activation='softmax', name='output')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721bec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model dengan optimizer, loss, dan metrics\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.TopKCategoricalAccuracy(k=2, name='top2_accuracy'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# tampilkan summary arsitektur model\n",
    "model.summary()\n",
    "\n",
    "# hitung total parameter\n",
    "total_params = model.count_params()\n",
    "trainable_params = np.sum([tf.size(v).numpy() for v in model.trainable_weights])\n",
    "non_trainable_params = np.sum([tf.size(v).numpy() for v in model.non_trainable_weights])\n",
    "\n",
    "print(f\"\\ntotal parameters: {total_params:,}\")\n",
    "print(f\"trainable parameters: {trainable_params:,}\")\n",
    "print(f\"non-trainable parameters: {non_trainable_params:,}\")\n",
    "\n",
    "print(\"\\nperbandingan arsitektur:\")\n",
    "print(\"  arsitektur lama: flatten + dense → banyak parameter → prone to overfitting\")\n",
    "print(\"  arsitektur baru: global avg pooling + dense → less parameter → better generalization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b992a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup callbacks untuk optimalisasi training \n",
    "\n",
    "# direktori untuk menyimpan checkpoint dan logs\n",
    "checkpoint_dir = Path('../checkpoints')\n",
    "log_dir = Path('../logs')\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 1. model checkpoint: simpan model terbaik berdasarkan validation accuracy\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=str(checkpoint_dir / 'best_model.keras'),\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 2. early stopping: stop training jika tidak ada improvement\n",
    "# patience=7\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=7,\n",
    "    mode='max',\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 3. reduce learning rate: jika stuck di plateau, kurangi learning speed\n",
    "# fine tuning\n",
    "reduce_lr_callback = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,  \n",
    "    patience=3,\n",
    "    min_lr=1e-7,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 4. tensorboard: logging untuk visualisasi training progress\n",
    "tensorboard_callback = TensorBoard(\n",
    "    log_dir=str(log_dir),\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    write_images=False,\n",
    "    update_freq='epoch'\n",
    ")\n",
    "\n",
    "callbacks_list = [\n",
    "    checkpoint_callback,\n",
    "    early_stopping_callback,\n",
    "    reduce_lr_callback,\n",
    "    tensorboard_callback\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73defa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model\n",
    "# hitung steps per epoch\n",
    "steps_per_epoch = train_generator.samples // BATCH_SIZE\n",
    "validation_steps = valid_generator.samples // BATCH_SIZE\n",
    "\n",
    "print(f\"steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"validation steps: {validation_steps}\\n\")\n",
    "\n",
    "# mulai training dengan class weights untuk handle imbalanced data\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks_list,\n",
    "    class_weight=class_weights_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\ntraining selesai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169d2006",
   "metadata": {},
   "source": [
    "## 6. Evaluasi Model\n",
    "\n",
    "Evaluasi dilakukan untuk mengukur performa model pada data yang belum pernah dilihat sebelumnya. Metrik evaluasi mencakup accuracy, precision, recall, dan f1-score. Visualisasi confusion matrix akan membantu mengidentifikasi pola kesalahan prediksi antar kelas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f666ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisasi training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# plot 1: accuracy\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(history.history['accuracy'], label='train accuracy', linewidth=2)\n",
    "ax1.plot(history.history['val_accuracy'], label='validation accuracy', linewidth=2)\n",
    "ax1.set_title('training dan validation accuracy', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# plot 2: loss\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(history.history['loss'], label='train loss', linewidth=2)\n",
    "ax2.plot(history.history['val_loss'], label='validation loss', linewidth=2)\n",
    "ax2.set_title('training dan validation loss', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.set_ylabel('loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# plot 3: precision\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(history.history['precision'], label='train precision', linewidth=2)\n",
    "ax3.plot(history.history['val_precision'], label='validation precision', linewidth=2)\n",
    "ax3.set_title('training dan validation precision', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('epoch')\n",
    "ax3.set_ylabel('precision')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# plot 4: recall\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(history.history['recall'], label='train recall', linewidth=2)\n",
    "ax4.plot(history.history['val_recall'], label='validation recall', linewidth=2)\n",
    "ax4.set_title('training dan validation recall', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('epoch')\n",
    "ax4.set_ylabel('recall')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafdfe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analisis overfitting/underfitting\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "acc_gap = final_train_acc - final_val_acc\n",
    "\n",
    "print(\"\\nanalisis overfitting/underfitting:\")\n",
    "print(f\"final training accuracy: {final_train_acc:.4f}\")\n",
    "print(f\"final validation accuracy: {final_val_acc:.4f}\")\n",
    "print(f\"accuracy gap: {acc_gap:.4f}\")\n",
    "\n",
    "if acc_gap < 0.05:\n",
    "    print(\"\\nkesimpulan: model well-fitted\")\n",
    "    print(\"gap antara training dan validation accuracy sangat kecil, menunjukkan generalisasi yang baik\")\n",
    "elif acc_gap < 0.10:\n",
    "    print(\"\\nkesimpulan: model slight overfitting\")\n",
    "    print(\"gap antara training dan validation accuracy masih dalam batas wajar untuk model deep learning\")\n",
    "else:\n",
    "    print(\"\\nkesimpulan: model overfitting\")\n",
    "    print(\"perlu tambahan regularisasi atau data augmentation untuk meningkatkan generalisasi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9360aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluasi model pada test set\n",
    "print(\"\\nevaluasi model pada test set...\")\n",
    "\n",
    "test_loss, test_accuracy, test_top2_accuracy, test_precision, test_recall = model.evaluate(\n",
    "    test_generator,\n",
    "    steps=test_generator.samples // BATCH_SIZE,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# hitung f1-score\n",
    "test_f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "\n",
    "print(\"\\nhasil evaluasi pada test set:\")\n",
    "print(f\"test loss: {test_loss:.4f}\")\n",
    "print(f\"test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"test top-2 accuracy: {test_top2_accuracy:.4f}\")\n",
    "print(f\"test precision: {test_precision:.4f}\")\n",
    "print(f\"test recall: {test_recall:.4f}\")\n",
    "print(f\"test f1-score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7808215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediksi pada test set untuk confusion matrix dan classification report\n",
    "print(\"\\nmembuat prediksi pada test set...\")\n",
    "\n",
    "# reset generator untuk memastikan urutan yang konsisten\n",
    "test_generator.reset()\n",
    "\n",
    "# prediksi\n",
    "y_pred_proba = model.predict(test_generator, steps=test_generator.samples // BATCH_SIZE, verbose=1)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# ambil true labels\n",
    "y_true = test_generator.classes[:len(y_pred)]\n",
    "\n",
    "print(f\"\\njumlah prediksi: {len(y_pred)}\")\n",
    "print(f\"jumlah true labels: {len(y_true)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f05ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# visualisasi confusion matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    cbar_kws={'label': 'jumlah prediksi'}\n",
    ")\n",
    "plt.title('confusion matrix - test set', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('predicted label', fontsize=12)\n",
    "plt.ylabel('true label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197af129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized confusion matrix (persentase)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(\n",
    "    cm_normalized, \n",
    "    annot=True, \n",
    "    fmt='.2%', \n",
    "    cmap='RdYlGn',\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    cbar_kws={'label': 'persentase', 'format': '%.0f%%'},\n",
    "    vmin=0,\n",
    "    vmax=1\n",
    ")\n",
    "plt.title('normalized confusion matrix - test set', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('predicted label', fontsize=12)\n",
    "plt.ylabel('true label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b72cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report detail per kelas\n",
    "print(\"\\nclassification report:\")\n",
    "report = classification_report(\n",
    "    y_true, \n",
    "    y_pred, \n",
    "    target_names=class_names,\n",
    "    digits=4\n",
    ")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf6568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisasi per-class accuracy\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_true, y_pred, labels=range(len(class_names))\n",
    ")\n",
    "\n",
    "# buat dataframe untuk visualisasi\n",
    "metrics_df = pd.DataFrame({\n",
    "    'class': class_names,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1-score': f1,\n",
    "    'support': support\n",
    "})\n",
    "\n",
    "# visualisasi metrik per kelas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# plot 1: precision per kelas\n",
    "ax1 = axes[0, 0]\n",
    "metrics_df.plot(x='class', y='precision', kind='bar', ax=ax1, color='steelblue', legend=False)\n",
    "ax1.set_title('precision per kelas', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('kelas nominal rupiah')\n",
    "ax1.set_ylabel('precision')\n",
    "ax1.set_ylim([0, 1.1])\n",
    "ax1.axhline(y=test_precision, color='r', linestyle='--', label=f'rata-rata: {test_precision:.3f}')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# plot 2: recall per kelas\n",
    "ax2 = axes[0, 1]\n",
    "metrics_df.plot(x='class', y='recall', kind='bar', ax=ax2, color='lightcoral', legend=False)\n",
    "ax2.set_title('recall per kelas', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('kelas nominal rupiah')\n",
    "ax2.set_ylabel('recall')\n",
    "ax2.set_ylim([0, 1.1])\n",
    "ax2.axhline(y=test_recall, color='r', linestyle='--', label=f'rata-rata: {test_recall:.3f}')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# plot 3: f1-score per kelas\n",
    "ax3 = axes[1, 0]\n",
    "metrics_df.plot(x='class', y='f1-score', kind='bar', ax=ax3, color='lightgreen', legend=False)\n",
    "ax3.set_title('f1-score per kelas', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('kelas nominal rupiah')\n",
    "ax3.set_ylabel('f1-score')\n",
    "ax3.set_ylim([0, 1.1])\n",
    "ax3.axhline(y=test_f1, color='r', linestyle='--', label=f'rata-rata: {test_f1:.3f}')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# plot 4: support per kelas\n",
    "ax4 = axes[1, 1]\n",
    "metrics_df.plot(x='class', y='support', kind='bar', ax=ax4, color='orange', legend=False)\n",
    "ax4.set_title('jumlah sampel per kelas (support)', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('kelas nominal rupiah')\n",
    "ax4.set_ylabel('jumlah sampel')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97096567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analisis kesalahan prediksi\n",
    "misclassified_indices = np.where(y_pred != y_true)[0]\n",
    "print(f\"\\njumlah kesalahan klasifikasi: {len(misclassified_indices)}\")\n",
    "print(f\"persentase kesalahan: {len(misclassified_indices)/len(y_true)*100:.2f}%\")\n",
    "\n",
    "# tampilkan beberapa contoh kesalahan klasifikasi\n",
    "if len(misclassified_indices) > 0:\n",
    "    print(\"\\ncontoh kesalahan klasifikasi:\")\n",
    "    \n",
    "    sample_errors = np.random.choice(misclassified_indices, min(10, len(misclassified_indices)), replace=False)\n",
    "    \n",
    "    for idx in sample_errors:\n",
    "        true_class = class_names[y_true[idx]]\n",
    "        pred_class = class_names[y_pred[idx]]\n",
    "        confidence = y_pred_proba[idx][y_pred[idx]] * 100\n",
    "        print(f\"true: {true_class:15} | predicted: {pred_class:15} | confidence: {confidence:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de600c6",
   "metadata": {},
   "source": [
    "## 7. Simpan Model\n",
    "\n",
    "Model yang telah dilatih akan disimpan dalam berbagai format untuk memudahkan deployment dan penggunaan di berbagai platform. Format yang tersedia termasuk keras native format, h5 untuk kompatibilitas backward, dan savedmodel untuk tensorflow serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a8c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simpan model dalam berbagai format\n",
    "model.save('../checkpoints/rupiah_classification_final.keras')\n",
    "print(\"model berhasil disimpan dalam format keras\")\n",
    "\n",
    "model.save('../checkpoints/rupiah_classification_final.h5')\n",
    "print(\"model berhasil disimpan dalam format h5\")\n",
    "\n",
    "model.save('../checkpoints/rupiah_classification_savedmodel')\n",
    "print(\"model berhasil disimpan dalam format savedmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5db1a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simpan metadata model untuk referensi\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "metadata = {\n",
    "    'model_name': 'Rupiah Classification CNN',\n",
    "    'version': '1.0',\n",
    "    'created_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'architecture': 'Custom CNN',\n",
    "    'input_shape': list(IMG_SIZE) + [3],\n",
    "    'num_classes': len(class_names),\n",
    "    'class_names': class_names,\n",
    "    'total_parameters': int(total_params),\n",
    "    'trainable_parameters': int(trainable_params),\n",
    "    'training': {\n",
    "        'epochs_trained': len(history.history['loss']),\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'optimizer': 'Adam',\n",
    "        'loss_function': 'categorical_crossentropy'\n",
    "    },\n",
    "    'performance': {\n",
    "        'train_accuracy': float(final_train_acc),\n",
    "        'val_accuracy': float(final_val_acc),\n",
    "        'test_accuracy': float(test_accuracy),\n",
    "        'test_precision': float(test_precision),\n",
    "        'test_recall': float(test_recall),\n",
    "        'test_f1_score': float(test_f1)\n",
    "    },\n",
    "    'dataset': {\n",
    "        'train_samples': int(train_generator.samples),\n",
    "        'valid_samples': int(valid_generator.samples),\n",
    "        'test_samples': int(test_generator.samples),\n",
    "        'total_samples': int(train_generator.samples + valid_generator.samples + test_generator.samples)\n",
    "    }\n",
    "}\n",
    "\n",
    "# tampilkan metadata\n",
    "print(\"\\nmetadata model:\")\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f098e6a",
   "metadata": {},
   "source": [
    "## 8. Testing Prediksi pada Gambar Baru\n",
    "\n",
    "Tahap akhir adalah menguji model pada gambar-gambar baru untuk memverifikasi kemampuan prediksi dalam kondisi real-world. Fungsi prediksi yang dibuat dapat dengan mudah diintegrasikan ke dalam aplikasi produksi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d389c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fungsi untuk prediksi gambar tunggal\n",
    "def predict_rupiah(image_path, model, class_names):\n",
    "    \"\"\"\n",
    "    melakukan prediksi pada gambar tunggal\n",
    "    \n",
    "    args:\n",
    "        image_path: path ke file gambar\n",
    "        model: model yang telah dilatih\n",
    "        class_names: list nama kelas\n",
    "    \n",
    "    returns:\n",
    "        tuple: (predicted_class, confidence, all_probabilities)\n",
    "    \"\"\"\n",
    "    # load dan preprocess gambar\n",
    "    img = load_img(image_path, target_size=IMG_SIZE)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = img_array / 255.0\n",
    "    \n",
    "    # prediksi\n",
    "    predictions = model.predict(img_array, verbose=0)\n",
    "    predicted_class_idx = np.argmax(predictions[0])\n",
    "    predicted_class = class_names[predicted_class_idx]\n",
    "    confidence = predictions[0][predicted_class_idx] * 100\n",
    "    \n",
    "    return predicted_class, confidence, predictions[0]\n",
    "\n",
    "print(\"fungsi predict_rupiah berhasil didefinisikan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaa37c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing prediksi pada sampel gambar dari test set\n",
    "print(\"\\ntesting prediksi pada sampel gambar random dari test set...\")\n",
    "\n",
    "# ambil sampel gambar random dari setiap kelas\n",
    "sample_images_paths = []\n",
    "for class_name in class_names:\n",
    "    class_path = test_dir / class_name\n",
    "    images = list(class_path.glob('*'))\n",
    "    if images:\n",
    "        sample_images_paths.append(np.random.choice(images))\n",
    "\n",
    "# visualisasi prediksi\n",
    "fig, axes = plt.subplots(4, 3, figsize=(15, 20))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, img_path in enumerate(sample_images_paths[:12]):\n",
    "    # prediksi\n",
    "    predicted_class, confidence, probabilities = predict_rupiah(img_path, model, class_names)\n",
    "    \n",
    "    # true label dari path\n",
    "    true_class = img_path.parent.name\n",
    "    \n",
    "    # load gambar untuk visualisasi\n",
    "    img = Image.open(img_path)\n",
    "    \n",
    "    # plot\n",
    "    ax = axes[idx]\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # warna title berdasarkan prediksi benar/salah\n",
    "    color = 'green' if predicted_class == true_class else 'red'\n",
    "    \n",
    "    title = f'true: {true_class}\\npred: {predicted_class}\\nconf: {confidence:.1f}%'\n",
    "    ax.set_title(title, fontsize=10, fontweight='bold', color=color)\n",
    "\n",
    "plt.suptitle('contoh prediksi pada test set\\n(hijau = benar, merah = salah)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b5bbc",
   "metadata": {},
   "source": [
    "## 9. Kesimpulan & Ringkasan\n",
    "\n",
    "Ringkasan lengkap dari seluruh pipeline klasifikasi uang rupiah, mencakup analisis dataset, desain arsitektur, hasil evaluasi, serta rekomendasi untuk pengembangan lebih lanjut. Bagian ini memberikan gambaran menyeluruh tentang kualitas dan keterbatasan model yang telah dibangun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640f15a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ringkasan lengkap proyek\n",
    "print(\"\\nRINGKASAN PROYEK KLASIFIKASI UANG RUPIAH DENGAN CNN\\n\")\n",
    "\n",
    "print(\"Analisis Dataset:\")\n",
    "print(f\"  total kelas: {len(class_names)}\")\n",
    "print(f\"  total gambar: {df_counts['total'].sum()}\")\n",
    "print(f\"    - training: {df_counts['train'].sum()} ({df_counts['train'].sum()/df_counts['total'].sum()*100:.1f}%)\")\n",
    "print(f\"    - validation: {df_counts['valid'].sum()} ({df_counts['valid'].sum()/df_counts['total'].sum()*100:.1f}%)\")\n",
    "print(f\"    - testing: {df_counts['test'].sum()} ({df_counts['test'].sum()/df_counts['total'].sum()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n  distribusi data training:\")\n",
    "print(f\"    - rata-rata per kelas: {train_mean:.0f} gambar\")\n",
    "print(f\"    - std deviasi: {train_std:.0f}\")\n",
    "print(f\"    - range: {df_counts['train'].min()} - {df_counts['train'].max()}\")\n",
    "\n",
    "print(f\"\\n  kelas dengan data terbatas:\")\n",
    "for idx, row in imbalanced_classes.iterrows():\n",
    "    print(f\"    - {idx}: {row['train']} gambar\")\n",
    "\n",
    "print(\"\\nArsitektur Model:\")\n",
    "print(\"  tipe: custom convolutional neural network\")\n",
    "print(\"  jumlah convolutional blocks: 5\")\n",
    "print(\"  fully connected layers: 2\")\n",
    "print(f\"  total parameters: {total_params:,}\")\n",
    "print(f\"    - trainable: {int(trainable_params):,}\")\n",
    "print(f\"    - non-trainable: {int(non_trainable_params):,}\")\n",
    "print(f\"  input shape: {IMG_SIZE[0]}x{IMG_SIZE[1]}x3\")\n",
    "print(f\"  output classes: {len(class_names)}\")\n",
    "\n",
    "print(\"\\nKonfigurasi Training:\")\n",
    "print(f\"  optimizer: Adam (lr={LEARNING_RATE})\")\n",
    "print(f\"  loss function: categorical crossentropy\")\n",
    "print(f\"  batch size: {BATCH_SIZE}\")\n",
    "print(f\"  epochs trained: {len(history.history['loss'])}\")\n",
    "print(\"  callbacks: model checkpoint, early stopping, reduce lr, tensorboard\")\n",
    "print(\"  data augmentation: rotation, shift, shear, zoom, flip, brightness\")\n",
    "print(\"  regularization: dropout, batch normalization, class weights\")\n",
    "\n",
    "print(\"\\nHasil Evaluasi:\")\n",
    "print(f\"  training accuracy: {final_train_acc:.4f} ({final_train_acc*100:.2f}%)\")\n",
    "print(f\"  validation accuracy: {final_val_acc:.4f} ({final_val_acc*100:.2f}%)\")\n",
    "print(f\"  test accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"  test precision: {test_precision:.4f} ({test_precision*100:.2f}%)\")\n",
    "print(f\"  test recall: {test_recall:.4f} ({test_recall*100:.2f}%)\")\n",
    "print(f\"  test f1-score: {test_f1:.4f} ({test_f1*100:.2f}%)\")\n",
    "print(f\"  accuracy gap (train-val): {acc_gap:.4f}\")\n",
    "\n",
    "if acc_gap < 0.05:\n",
    "    print(\"  status: model well-fitted\")\n",
    "elif acc_gap < 0.10:\n",
    "    print(\"  status: slight overfitting\")\n",
    "else:\n",
    "    print(\"  status: overfitting detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c2cdc1",
   "metadata": {},
   "source": [
    "### Analisis Dataset & Rekomendasi\n",
    "\n",
    "**Kualitas Dataset**\n",
    "\n",
    "Dataset yang digunakan terdiri dari 11 kelas dengan total sekitar 3.500+ gambar. Distribusi data menunjukkan ketidakseimbangan yang cukup signifikan, dimana koin_100 hanya memiliki 44 gambar dan koin_200 sebanyak 66 gambar, sementara kelas lainnya memiliki 300-500 gambar. Ketidakseimbangan ini telah dimitigasi dengan menggunakan class weights dan data augmentation yang adaptif.\n",
    "\n",
    "**Kecukupan Dataset**\n",
    "\n",
    "Secara umum, dataset ini sudah cukup untuk membangun model dasar yang akurat. Namun, untuk meningkatkan robustness terutama pada kelas minority, diperlukan penambahan data. Model yang dibangun dengan arsitektur CNN custom, dilengkapi dengan regularisasi yang memadai, mampu menghasilkan performa yang baik meskipun dengan keterbatasan data pada beberapa kelas.\n",
    "\n",
    "**Rekomendasi Improvement**\n",
    "\n",
    "1. **Augmentasi Data Kelas Minority**\n",
    "   - Tambahkan minimal 200-300 gambar untuk koin_100 dan koin_200\n",
    "   - Pertimbangkan penggunaan synthetic data generation atau web scraping untuk memperkaya dataset\n",
    "   - Lakukan manual collection dengan berbagai kondisi pencahayaan dan sudut pengambilan\n",
    "\n",
    "2. **Optimasi Arsitektur**\n",
    "   - Eksplorasi arsitektur yang lebih dalam seperti ResNet atau EfficientNet\n",
    "   - Implementasi transfer learning dengan pre-trained weights dari ImageNet\n",
    "   - Gunakan ensemble multiple models untuk meningkatkan robustness dan mengurangi variance\n",
    "\n",
    "3. **Advanced Data Augmentation**\n",
    "   - Implementasi Cutout, MixUp, atau CutMix untuk regularisasi yang lebih kuat\n",
    "   - Tambahkan Gaussian noise dan blur untuk simulasi kondisi real-world\n",
    "   - Gunakan perspective transformation untuk variasi sudut pengambilan gambar\n",
    "\n",
    "4. **Hyperparameter Optimization**\n",
    "   - Lakukan systematic hyperparameter tuning dengan Optuna atau Ray Tune\n",
    "   - Eksperimen dengan learning rate scheduling yang lebih kompleks seperti cosine annealing\n",
    "   - Implementasi test-time augmentation untuk meningkatkan akurasi inference\n",
    "\n",
    "5. **Production Deployment**\n",
    "   - Konversi model ke TensorFlow Lite untuk deployment di mobile devices\n",
    "   - Implementasi quantization untuk mengurangi ukuran model tanpa mengorbankan akurasi\n",
    "   - Optimasi untuk edge devices agar dapat melakukan real-time inference\n",
    "\n",
    "**Kesimpulan Akhir**\n",
    "\n",
    "Model yang telah dibangun menunjukkan performa yang solid dengan arsitektur CNN custom. Kombinasi dari data augmentation, batch normalization, dropout, L2 regularization, dan class weights berhasil menghasilkan model yang cukup robust terhadap overfitting. Untuk deployment production, implementasi dari rekomendasi di atas akan sangat membantu meningkatkan generalisasi model dan performa pada edge cases, terutama untuk kelas-kelas dengan sampel terbatas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
